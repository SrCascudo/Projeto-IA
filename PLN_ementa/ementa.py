# -*- coding: utf-8 -*-
"""ementa.ipynb

Automatically generated by Colaboratory.

Original file is located at
    https://colab.research.google.com/drive/1PkP0aSB5s3uRg-E71LtuBtJ2sGlsR-vH
"""

!pip install youtube_dl
!pip install SpeechRecognition as sr
!pip install nltk
!pip install fuzzywuzzy
!pip install python-Levenshtein
!pip install beautifulsoup4

"""> # Imports"""

import os
import nltk
import string
import requests
import youtube_dl
from fuzzywuzzy import fuzz
from bs4 import BeautifulSoup
import speech_recognition as sr

"""> # Baixar Video do YouTube"""

diretorio="/content/ementa/resources/"
path_name= diretorio + "filosofia"
typefile = "wav"

ydl_opts = {
        'format': 'bestaudio/best',
        'postprocessors': [{
            'key': 'FFmpegExtractAudio',
            'preferredcodec': typefile,
            'preferredquality': '192',
        }],
        'outtmpl': path_name + '.%(ext)s',
    }

with youtube_dl.YoutubeDL(params=ydl_opts, auto_init=True) as ydl:
  ydl.download(["https://www.youtube.com/watch?v=JixtlfwvhAA"])

path_name = path_name + '.' + typefile

"""> # Trascrever Audio


### Utilizando **speech_recognition** para transcrição do texto
"""

r = sr.Recognizer()
arquivo = sr.AudioFile(path_name)
with arquivo as source:
  r.adjust_for_ambient_noise(source)

  conversacao = ""
  ocorrencia = 0

  for i in range(0, int(source.DURATION)+1, 120):
    ocorrencia += 1
    print(str(ocorrencia)+'º Laço')
    audio = r.record(source, duration=120, offset=0)
    conversacao = conversacao + r.recognize_google(audio, language='pt-BR') + ' '

  print(conversacao)

"""> # Removendo todas as stopwords e potuações da transcrição"""

nltk.download('stopwords')
stopwords = nltk.corpus.stopwords.words('portuguese')
pontuacao = string.punctuation

texto = ""
for word in conversacao.split(): 
    if word.lower() not in stopwords:
        texto += word.lower() + ' '

print(texto)

"""> # Similaridade com texto de ementa"""

page = requests.get("http://dontpad.com/srcascudo")

soup = BeautifulSoup(page.content, 'html.parser')
textopage = str(soup.find_all('textarea')).replace('[<textarea id="text">','').replace('</textarea>]', '')

ementa = ""
for word in textopage.split(): 
    if word.lower() not in stopwords:
        ementa += word.lower() + ' '

fuzz.partial_ratio(texto, ementa)